{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gemini 1.5 Pro Python code samples\n",
        "\n"
      ],
      "metadata": {
        "id": "7yVV6txOmNMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| | |\n",
        "|-|-|\n",
        "|Author(s) | [Eric Dong](https://github.com/gericdong)|"
      ],
      "metadata": {
        "id": "1EExYZvij2ve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up environment"
      ],
      "metadata": {
        "id": "DbCqHcPvYQn0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "#### Install Vertex AI SDK for Python (Dev build)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFy3H3aPgx12",
        "tags": []
      },
      "outputs": [],
      "source": [
        "! pip3 install --upgrade --user --quiet google-cloud-aiplatform\n",
        "\n",
        "# ! pip3 install --force-reinstall --quiet git+https://github.com/googleapis/python-aiplatform.git@main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xep4W9lq-Z"
      },
      "source": [
        "#### Restart runtime (Colab only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRvKdaPDTznN",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15dc669c-130d-4053-b4ae-25a03220addb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "#### Authenticate your notebook environment (Colab only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyKGtVQjgx13",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "#### Set Google Cloud project information and initialize Vertex AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqwi-5ufWp_B",
        "tags": []
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"vertex-training-356201\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "project_id = PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code samples\n",
        "\n",
        "- Make best effort to make samples copy-paste-runnable\n",
        "- Set default parameter values when possible, only `project_id` is needed from users\n",
        "- Model output is non-deterministic, don't hard-code generated text in tests\n",
        "- Use `generativeaionvertexai` as a product prefix (as opposed to `aiplatform`)\n",
        "- Put region tags and all code inside a function, do not need to expose the function to users\n",
        "```\n",
        "def funct() -> str:\n",
        "  [START generativeaionvertexai-region-tag]\n",
        "  import vertexai\n",
        "  ...\n",
        "\n",
        "  [END generativeaionvertexai-region-tag]\n",
        "  return str\n",
        "```"
      ],
      "metadata": {
        "id": "kWEzNmMsnnhN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Audio understanding"
      ],
      "metadata": {
        "id": "UfT_0BsMVjTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/GoogleCloudPlatform/python-docs-samples/blob/main/generative_ai/gemini_audio.py\n",
        "\n",
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#    https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\n",
        "def summarize_audio(project_id: str) -> str:\n",
        "  # [START generativeaionvertexai_gemini_audio_summarization]\n",
        "\n",
        "  import vertexai\n",
        "  from vertexai.generative_models import GenerativeModel, Part\n",
        "\n",
        "  # TODO(developer): Update and un-comment below lines\n",
        "  # project_id = \"PROJECT_ID\"\n",
        "\n",
        "  vertexai.init(project=project_id, location=\"us-central1\")\n",
        "\n",
        "  model = GenerativeModel(\"gemini-1.5-pro-preview-0409\")\n",
        "\n",
        "  prompt = \"\"\"\n",
        "    Please provide a summary for the audio.\n",
        "    Provide chapter titles with timestamps, be concise and short, no need to provide chapter summaries.\n",
        "    Do not make up any information that is not part of the audio and do not be verbose.\n",
        "  \"\"\"\n",
        "\n",
        "  audio_file_uri = \"gs://cloud-samples-data/generative-ai/audio/pixel.mp3\"\n",
        "  audio_file = Part.from_uri(audio_file_uri, mime_type=\"audio/mpeg\")\n",
        "\n",
        "  contents = [audio_file, prompt]\n",
        "\n",
        "  response = model.generate_content(contents)\n",
        "  print(response.text)\n",
        "\n",
        "  # [END generativeaionvertexai_gemini_audio_summarization]\n",
        "  return response.text\n",
        "\n",
        "\n",
        "def transcript_audio(project_id: str) -> str:\n",
        "  # [START generativeaionvertexai_gemini_audio_transcription]\n",
        "\n",
        "  import vertexai\n",
        "  from vertexai.generative_models import GenerativeModel, Part\n",
        "\n",
        "  # TODO(developer): Update and un-comment below lines\n",
        "  # project_id = \"PROJECT_ID\"\n",
        "\n",
        "  vertexai.init(project=project_id, location=\"us-central1\")\n",
        "\n",
        "  model = GenerativeModel(\"gemini-1.5-pro-preview-0409\")\n",
        "\n",
        "  prompt = \"\"\"\n",
        "    Can you transcribe this interview, in the format of timecode, speaker, caption?\n",
        "    Use speaker A, speaker B, etc. to identify speakers.\n",
        "  \"\"\"\n",
        "\n",
        "  audio_file_uri = \"gs://cloud-samples-data/generative-ai/audio/pixel.mp3\"\n",
        "  audio_file = Part.from_uri(audio_file_uri, mime_type=\"audio/mpeg\")\n",
        "\n",
        "  contents = [audio_file, prompt]\n",
        "\n",
        "  response = model.generate_content(contents)\n",
        "  print(response.text)\n",
        "\n",
        "  # [END generativeaionvertexai_gemini_audio_transcription]\n",
        "  return response.text"
      ],
      "metadata": {
        "id": "PVnYRYOJV5-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/GoogleCloudPlatform/python-docs-samples/blob/main/generative_ai/test_gemini_examples.py\n",
        "\n",
        "# import gemini_audio\n",
        "\n",
        "def test_summarize_audio() -> None:\n",
        "    text = summarize_audio(PROJECT_ID)\n",
        "    assert len(text) > 0\n",
        "\n",
        "def test_transcript_audio() -> None:\n",
        "    text = transcript_audio(PROJECT_ID)\n",
        "    assert len(text) > 0\n",
        "\n",
        "\n",
        "test_summarize_audio()\n",
        "test_transcript_audio()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgTDmvascYJU",
        "outputId": "1d34ef1a-0a33-47f0-d3df-efbe7a0be630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This audio is about the Pixel Feature Drops. \n",
            "\n",
            "Chapter Titles & Timestamps:\n",
            "\n",
            "*  Introduction (00:00)\n",
            "*  Importance of Feature Drops (01:49)\n",
            "*  January Feature Drop Highlights (02:48)\n",
            "*  March Feature Drop - Pixel Watch (03:41)\n",
            "*  March Feature Drop - Pixel Phone (05:58)\n",
            "*  March Feature Drop - Other Devices (07:41) \n",
            "*  Pixel Superfans Question (08:12)\n",
            "*  Favorite Feature Drop Features (08:23)\n",
            "*  Feature Drop Release Date (08:07)\n",
            "*  Outro (10:17) \n",
            "\n",
            "## Interview Transcription\n",
            "\n",
            "**00:00** | **Speaker A** | your devices are getting better over time and so we think about it across the entire portfolio from phones to watch to buds to tablet we get really excited about how we can tell a joint narrative across everything.\n",
            "\n",
            "**00:14** | **Speaker B** | Welcome to the made by Google podcast where we meet the people who work on the Google products you love. Here's your host Rasheed Finch. Today we're talking to Aisha Sharif and the Carlos Love, they're both product managers for various pixel devices and work on something that all the pixel owners love, the pixel feature drops.\n",
            "\n",
            "**00:35** | **Speaker B** | Aisha, which feature on your Pixel phone has been most transformative in your own life.\n",
            "\n",
            "**00:41** | **Speaker A** | So many features. I am a singer, so I actually think recorder transcription has been incredible because before I would record songs, I just like freestyle them, record them, type them up, but now with transcription it works so well even deciphering lyrics that are jumbled. I think that's huge.\n",
            "\n",
            "**00:58** | **Speaker B** | Amazing. The Carlos same question to you, but for your Pixel Watch of course, long time listeners will know you work on Pixel Watch. What has been the most transformative feature in your own life on Pixel Watch? \n",
            "\n",
            "**01:07** | **Speaker C** | I work on the fitness experiences and so for me it's definitely the ability to track my heart rate but specifically around the different heart rate targets and zone features that we've released. For me it's been super helpful, my background is in more football, track and field in terms of what I've done before and so using the heart rate features to really help me understand that I shouldn't be going as hard when I'm running, you know, leisurely two or three miles and helping me really tone that down a bit. It's actually been pretty transformative for me to see how things like my resting heart rate have changed due to that feature. \n",
            "\n",
            "**01:43** | **Speaker B** | Amazing. And Aisha, I know we spend a lot of time and energy on feature drops within the Pixel team, why are they so important to us?\n",
            "\n",
            "**01:49** | **Speaker A** | So, exactly what Carlos said, they're important to this narrative that your devices are getting better over time and so we think about it across the entire portfolio from phones to watch to buds to tablet. We get really excited about how we can tell a joint narrative across everything.\n",
            "\n",
            "**02:12** | **Speaker A** | The other part is with our Pixel 8 and 8 Pro, and I'm still so excited about this, we have seven years of OS updates, security updates and feature drops. And so feature drops just pair so nicely into this narrative of how your devices are getting better over time and they'll continue to get better over time.\n",
            "\n",
            "**02:28** | **Speaker B** | Yeah, we'll still be talking about Pixel 8 and Pixel 8 Pro in 2030 with those seven years of software updates and I promise we'll have an episode on that shortly. Now, the March feature drop is upon us, but I just wanted to look back to the last one, first one from January. Aisha, could you tell us some of the highlights from the January one that just launched?\n",
            "\n",
            "**02:48** | **Speaker A** | So it was one of the few times that we've done a software drop with hardware as well. So it was really exciting to get that new mint color out on Pixel 8 and 8 Pro. We also had the body temperature sensor launch in the US, so now you're able to actually just with like a scan of your forehead, get your body temp, which is huge. And then a ton of AI enhancements, circle to search, came to Pixel 8 and 8 Pro, so you can search from anywhere. One of my favorites, photo emoji, so now you can use photos that you have in your album and react to messages with them. Most random, I was hosting a donut ice cream party, and literally had a picture of a donut ice cream sandwich that I used to react to messages. Love those little random, random reactions that you can put out there. \n",
            "\n",
            "**03:30** | **Speaker B** | Amazing. And and that was just two months ago. Now we're upon the March feature drop already. There's one for Pixel phones, then one for Pixel Watches as well. Let's start now with the watch, Carlos, what's new in March?\n",
            "\n",
            "**03:41** | **Speaker C** | The big story for us is that not only are we going to make sure that all of your watches get better over time but specifically bringing things to previous gen watches. So, we had some features that launch on the Pixel Watch 2, and in this feature drop, we're bringing those features to the Pixel Watch 1. Some of the things specifically are looking at are our pace features, the thing I mentioned earlier around our heart rate features as well are coming to the Pixel Watch 1. That allows you to to kind of set those different settings to target a pace that you want to stay within and get those notifications while you're working out if you're ahead or above that pace, and similar with the heart rate zones as well. \n",
            "\n",
            "**04:04** | **Speaker C** | We're also bringing activity recognition to Pixel Watch 1 and users in addition to auto pause will be able to leverage activity recognition for them to start their workouts in case they forget to actually start it on their own, as well as they'll get a notification to help them stop their workouts in case they forget to end their workout when they're actually done. \n",
            "\n",
            "**04:27** | **Speaker C** | Outside of workouts, another feature that's coming in this feature drop is really around the Fitbit relax app, something that folks enjoy from Pixel Watch 2, we're also bringing that there so people can jump into, you know, take a relaxed full moment and work through breathing exercises right on their wrist.\n",
            "\n",
            "**04:53** | **Speaker B** | Let's get to the March feature drop on the phone side now, Aisha, what's new for for Pixel phone users? \n",
            "\n",
            "**04:58** | **Speaker A** | So I'm going to second the sentiment that Carlos shared with March really being around devices being made to last. So, Pixel Watch 1 getting features from Pixel Watch 2, we're seeing that on the phone side as well. So circle to search will be expanding to Pixel 7 and 7 Pro. We're also seeing 10-bit HDR move outside of just the camera, but it'll be available in Instagram so you can take really high quality reels. We also have partial screen sharing. \n",
            "\n",
            "**05:24** | **Speaker A** | So instead of having to share your entire screen of your phone or your tablet when you're in a meeting or you might be casting, now you can just share a specific app. Just huge for privacy.\n",
            "\n",
            "**05:34** | **Speaker B** | Those are some amazing updates in the March feature drop. Could you tell us a little bit more about, is there any news maybe for the rest of the portfolio as well?\n",
            "\n",
            "**05:41** | **Speaker A** | Yes, so partial screen sharing is coming to tablet. We're also seeing Docs markup come to tablet, so you can actually just directly, what it sounds like, mark up docs, um, but draw on them, take notes in them and you can do that on your phone as well. And then another one that's amazing, Bluetooth connection is getting even better. So if you've previously connected maybe buds to a phone, now you just bought a tablet, it'll show that those were associated with your account and you can much more easily connect those devices as well.\n",
            "\n",
            "**06:09** | **Speaker B** | There is a part of this conversation I'm looking forward to most, which is asking a question from the Pixel super fans community, they're getting the opportunity each episode to ask a question. And today's question comes from Casey Carpenter and they're asking, what drives your choice of new software in releases, which is a good one. So you mentioned now, and Carlos will start with you, you mentioned a set of features coming to the first generation Pixel Watch, like how do you sort of decide which ones make to cut this time, which one maybe come next time, how does that work?\n",
            "\n",
            "**06:40** | **Speaker C** | For us, we really think about the core principle of we want to make sure that these devices are able to continue to get better. And we know that there has been improvements from Pixel Watch 2 and so in this case it's about making sure that we we bring those features to the Pixel Watch 1 as well. \n",
            "\n",
            "**06:51** | **Speaker C** | Obviously we'd like to think about can it actually happen, sometimes there may be new sensors or things like that on a newer generation that are just make some features not possible for a previous gen, but in the event that we can bring it back we always strive to do that, especially when we know that we have a lot of good reception from those features and users that have kind of given us the feedback on the helpfulness of them. What are the things that the users really value and really lean into that as helping shape how we think about what comes next.\n",
            "\n",
            "**07:24** | **Speaker B** | Aisha, Carlos mentioned user feedback as a part of deciding what's coming in a feature drop. How important is that in making all of the decisions?\n",
            "\n",
            "**07:32** | **Speaker A** | I think user feedback is huge to everything that we do across devices. So in our drops we're always thinking about what improvements we can bring to people based on user feedback, but also based on what we're hearing. And so feature drops are a really great way to continue to enhance features that have already gone out and add improvements on top of them. It's also a way for us to introduce things that are completely new or like Carlos mentioned, take things that were on newer devices and bring them back to older devices.\n",
            "\n",
            "**07:58** | **Speaker B** | Now I'm sure there are a lot of people listening wondering when can they get their hands on these new features. When is the March feature drop actually landing on their devices? Any thoughts there?\n",
            "\n",
            "**08:07** | **Speaker A** | So the March feature drop, all these features will start rolling out today, March 4th.\n",
            "\n",
            "**08:12** | **Speaker B** | Now we've had many, many, many feature drops over the years. And I'm wondering are there any particular features that stand out to you that we launched in a feature drop? Maybe Aisha I can start with you.\n",
            "\n",
            "**08:23** | **Speaker A** | I think all of the call features have been incredibly helpful for me. So a couple of my favorites, call screen, we had an enhancement in December where you get contextual tips now, so if somebody's like leaving a package and you're in the middle of a meeting you can respond to that. Also direct my call is available for non-toll free numbers. So if you're calling a doctor's office that starts with just your local area code, now you can actually use direct my call on that, which is such a time saver as well. And clear calling, love that feature, especially when I'm trying to talk to my mom and she's talking to a million people around her as as we're trying to have our conversation. So, all incredibly, incredibly helpful features.\n",
            "\n",
            "**08:59** | **Speaker B** | That's amazing, such staples of the Pixel family right now and they all came through a feature drop. The Carlos, of course Pixel Watch has had several feature drops as well. Any favorite in there for you?\n",
            "\n",
            "**09:08** | **Speaker C** | Yeah, I have a couple outside of the things that are launching right now. I think one was when we released the SPO2 feature in a feature drop. That was one of the things that we heard and knew from the original launch of Pixel Watch 1 that people were excited and looking forward to. So it measures your oxygen saturation, you can wear your watch when you sleep and overnight we'll we'll measure that SPO2 oxygen saturation while you're sleeping. So that was an exciting one. We got a lot of good feedback on being able to to release that and bring that to the Pixel Watch 1 initially. \n",
            "\n",
            "**09:34** | **Speaker C** | Oh actually, one of the things that's happening in this latest feature drop with the relax app, I just really love the attention and the design around the breathing animations. And so something that folks should definitely check out is, you know, the team that put a lot of good work into just thinking about the pace at which that animation occurs. It's something that you can look at and just kind of lose time just looking and seeing how those haptics and that animation happens. \n",
            "\n",
            "**10:06** | **Speaker B** | Amazing. It's always the little things that make it extra special, right? \n",
            "\n",
            "**10:09** | **Speaker C** | Absolutely. \n",
            "\n",
            "**10:09** | **Speaker B** | That's perfect. Aisha, Carlos, thank you so much for making Christmas come early once again and we're all looking forward to the feature drop in March.\n",
            "\n",
            "**10:17** | **Speaker A** | Thank you.\n",
            "\n",
            "**10:18** | **Speaker C** | Thank you. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Video with audio"
      ],
      "metadata": {
        "id": "S6xKfQTffBN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/GoogleCloudPlatform/python-docs-samples/blob/main/generative_ai/gemini_video_audio.py\n",
        "\n",
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#    https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\n",
        "def analyze_video_with_audio(project_id: str) -> str:\n",
        "  # [START generativeaionvertexai_gemini_video_with_audio]\n",
        "\n",
        "  import vertexai\n",
        "  from vertexai.generative_models import GenerativeModel, Part\n",
        "\n",
        "  # TODO(developer): Update and un-comment below lines\n",
        "  # project_id = \"PROJECT_ID\"\n",
        "\n",
        "  vertexai.init(project=project_id, location=\"us-central1\")\n",
        "\n",
        "  model = GenerativeModel(\"gemini-1.5-pro-preview-0409\")\n",
        "\n",
        "  prompt = \"\"\"\n",
        "    Provide a description of the video.\n",
        "    The description should also contain anything important which people say in the video.\n",
        "  \"\"\"\n",
        "\n",
        "  video_file_uri = \"gs://cloud-samples-data/generative-ai/video/pixel8.mp4\"\n",
        "  video_file = Part.from_uri(video_file_uri, mime_type=\"video/mp4\")\n",
        "\n",
        "  contents = [video_file, prompt]\n",
        "\n",
        "  response = model.generate_content(contents)\n",
        "  print(response.text)\n",
        "\n",
        "  # [END generativeaionvertexai_gemini_video_with_audio]\n",
        "  return response.text"
      ],
      "metadata": {
        "id": "ycXqXGzcVgrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/GoogleCloudPlatform/python-docs-samples/blob/main/generative_ai/test_gemini_examples.py\n",
        "\n",
        "# import gemini_video_audio\n",
        "\n",
        "def test_analyze_video_with_audio() -> None:\n",
        "    text = analyze_video_with_audio(PROJECT_ID)\n",
        "    assert len(text) > 0\n",
        "\n",
        "test_analyze_video_with_audio()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJ5Z8AmgVguq",
        "outputId": "194c38e6-e140-4550-eb60-bc748c0f70bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The video is a night tour of Tokyo with a photographer named Saeka Shimada. She shows us different parts of the city and how they look different at night. She also demonstrates the new Pixel phone's \"Video Boost\" feature, which improves video quality in low-light conditions. The video ends with Saeka in Shibuya, a popular district in Tokyo known for its nightlife.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. All modalities"
      ],
      "metadata": {
        "id": "HeJsUOc-jdaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/GoogleCloudPlatform/python-docs-samples/blob/main/generative_ai/gemini_all_modalities.py\n",
        "\n",
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#    https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\n",
        "def analyze_all_modalities(project_id: str) -> str:\n",
        "  # [START generativeaionvertexai_gemini_all_modalities]\n",
        "\n",
        "  import vertexai\n",
        "  from vertexai.generative_models import GenerativeModel, Part\n",
        "\n",
        "  # TODO(developer): Update and un-comment below lines\n",
        "  # project_id = \"PROJECT_ID\"\n",
        "\n",
        "  vertexai.init(project=project_id, location=\"us-central1\")\n",
        "\n",
        "  model = GenerativeModel(\"gemini-1.5-pro-preview-0409\")\n",
        "\n",
        "  video_file_uri = \"gs://cloud-samples-data/generative-ai/video/behind_the_scenes_pixel.mp4\"\n",
        "  video_file = Part.from_uri(video_file_uri, mime_type=\"video/mp4\")\n",
        "\n",
        "  image_file_uri = \"gs://cloud-samples-data/generative-ai/image/a-man-and-a-dog.png\"\n",
        "  image_file = Part.from_uri(image_file_uri, mime_type=\"image/png\")\n",
        "\n",
        "  prompt = \"\"\"\n",
        "    Watch each frame in the video carefully and answer the questions.\n",
        "    Only base your answers strictly on what information is available in the video attached.\n",
        "    Do not make up any information that is not part of the video and do not be too\n",
        "    verbose, be to the point.\n",
        "\n",
        "    Questions:\n",
        "    - When is the moment in the image happening in the video? Provide a timestamp.\n",
        "    - What is the context of the moment and what does the narrator say about it?\n",
        "  \"\"\"\n",
        "\n",
        "  contents = [\n",
        "      video_file,\n",
        "      image_file,\n",
        "      prompt,\n",
        "  ]\n",
        "\n",
        "  response = model.generate_content(contents)\n",
        "  print(response.text)\n",
        "\n",
        "  # [END generativeaionvertexai_gemini_all_modalities]\n",
        "  return response.text"
      ],
      "metadata": {
        "id": "H1C3wGlBVgxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/GoogleCloudPlatform/python-docs-samples/blob/main/generative_ai/test_gemini_examples.py\n",
        "\n",
        "# import gemini_all_modalities\n",
        "\n",
        "def test_analyze_all_modalities() -> None:\n",
        "    text = analyze_all_modalities(PROJECT_ID)\n",
        "    assert len(text) > 0\n",
        "\n",
        "test_analyze_all_modalities()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8gkUBpDVg0m",
        "outputId": "54c8d04a-cf65-41ad-b68d-3f120d0c54ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- The image appears at the 0:49 mark. \n",
            "-  It's part of a montage of blurred video clips depicting significant moments in the relationship between the blind man and his girlfriend. The narrator says the AI feature on the Pixel phone allows the blind man to capture these moments.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. System instruction"
      ],
      "metadata": {
        "id": "cYJDh-6KnECU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/GoogleCloudPlatform/python-docs-samples/blob/main/generative_ai/gemini_system_instruction.py\n",
        "\n",
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#    https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\n",
        "def set_system_instruction(project_id: str) -> str:\n",
        "  # [START generativeaionvertexai_gemini_system_instruction]\n",
        "\n",
        "  import vertexai\n",
        "  from vertexai.generative_models import GenerativeModel\n",
        "\n",
        "  # TODO(developer): Update and un-comment below lines\n",
        "  # project_id = \"PROJECT_ID\"\n",
        "\n",
        "  vertexai.init(project=project_id, location=\"us-central1\")\n",
        "\n",
        "  model = GenerativeModel(\n",
        "      \"gemini-1.5-pro-preview-0409\",\n",
        "      system_instruction=[\n",
        "          \"You are a helpful language translator.\",\n",
        "          \"Your mission is to translate text in English to French.\",\n",
        "      ],\n",
        "  )\n",
        "\n",
        "  prompt = \"\"\"\n",
        "    User input: I like bagels.\n",
        "    Answer:\n",
        "  \"\"\"\n",
        "\n",
        "  contents = [prompt]\n",
        "\n",
        "  response = model.generate_content(contents)\n",
        "  print(response.text)\n",
        "\n",
        "  # [END generativeaionvertexai_gemini_system_instruction]\n",
        "  return response.text"
      ],
      "metadata": {
        "id": "oHcXpkavVg3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/GoogleCloudPlatform/python-docs-samples/blob/main/generative_ai/test_gemini_examples.py\n",
        "\n",
        "# import gemini_system_instruction\n",
        "\n",
        "def test_set_system_instruction() -> None:\n",
        "    text = set_system_instruction(PROJECT_ID)\n",
        "    assert len(text) > 0\n",
        "\n",
        "test_set_system_instruction()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCwN4rFftbhx",
        "outputId": "ceed38f2-5a01-42b0-bdcb-07c3a5e2d8c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "J'aime les bagels. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. PDF"
      ],
      "metadata": {
        "id": "oSstqGmPTGh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/GoogleCloudPlatform/python-docs-samples/blob/main/generative_ai/gemini_pdf_example.py\n",
        "\n",
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#    https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\n",
        "def analyze_pdf(project_id: str) -> str:\n",
        "    # [START generativeaionvertexai_gemini_pdf]\n",
        "\n",
        "    import vertexai\n",
        "    from vertexai.generative_models import GenerativeModel, Part\n",
        "\n",
        "    # TODO(developer): Update and un-comment below lines\n",
        "    # project_id = \"PROJECT_ID\"\n",
        "\n",
        "    vertexai.init(project=project_id, location=\"us-central1\")\n",
        "\n",
        "    model = GenerativeModel(\"gemini-1.5-pro-preview-0409\")\n",
        "\n",
        "    prompt = \"\"\"\n",
        "    You are a very professional document summarization specialist.\n",
        "    Please summarize the given document.\n",
        "    \"\"\"\n",
        "\n",
        "    pdf_file_uri = \"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\"\n",
        "    pdf_file = Part.from_uri(pdf_file_uri, mime_type=\"application/pdf\")\n",
        "    contents = [pdf_file, prompt]\n",
        "\n",
        "    response = model.generate_content(contents)\n",
        "    print(response.text)\n",
        "\n",
        "    # [END generativeaionvertexai_gemini_pdf]\n",
        "    return response.text"
      ],
      "metadata": {
        "id": "sXql-x68pJ93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/GoogleCloudPlatform/python-docs-samples/blob/main/generative_ai/test_gemini_examples.py\n",
        "\n",
        "# import gemini_pdf_example\n",
        "\n",
        "def test_gemini_pdf_example() -> None:\n",
        "    text = analyze_pdf(PROJECT_ID)\n",
        "    assert len(text) > 0\n",
        "\n",
        "test_gemini_pdf_example()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9gou2s-TUaK",
        "outputId": "2f4d07b3-b061-4494-ce8b-e14f2465af32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Gemini 1.5 Pro: Summary of Capabilities\n",
            "\n",
            "**Gemini 1.5 Pro** is a new, highly efficient multimodal AI model from Google DeepMind. It excels at understanding and reasoning over vast amounts of information across various formats, including text, video, audio, and code. \n",
            "\n",
            "Here's a breakdown of its key features:\n",
            "\n",
            "**Long-Context Understanding:**\n",
            "\n",
            "* **Unprecedented Scale:**  Gemini 1.5 Pro handles context lengths of **up to 10 million tokens**, exceeding existing models like Claude 2.1 (200k) and GPT-4 Turbo (128k) by a significant margin. This allows it to process large documents, multiple hours of video, and almost five days of audio recordings.\n",
            "* **Near-Perfect Recall:** It achieves near-perfect recall (>99%) on retrieval tasks across modalities, even when information is buried within millions of tokens.\n",
            "* **Reasoning and Understanding:** Beyond simple recall, Gemini 1.5 Pro demonstrates the ability to reason and understand relationships within long and complex contexts, enabling it to answer questions from lengthy documents and videos effectively.\n",
            "* **In-Context Learning:**  The model showcases impressive in-context learning abilities, such as learning to translate a new language (Kalamang) from a single reference grammar book and bilingual wordlist.\n",
            "\n",
            "**Multimodal Capabilities:**\n",
            "\n",
            "* **Text:**  Gemini 1.5 Pro excels in various text-based tasks, surpassing previous models in areas like math, science, reasoning, coding, multilingual understanding, and instruction following.\n",
            "* **Vision:** It demonstrates strong performance on image and video understanding benchmarks, particularly excelling in multimodal reasoning tasks.\n",
            "* **Audio:** The model achieves state-of-the-art results on automatic speech recognition and translation tasks, rivaling specialized models trained exclusively for speech processing.\n",
            "\n",
            "**Efficiency and Performance:**\n",
            "\n",
            "* **Reduced Training Compute:** Gemini 1.5 Pro utilizes a novel mixture-of-experts architecture and improved training infrastructure to achieve comparable quality to previous models while requiring significantly less training compute.\n",
            "* **Efficient Serving:** The model is designed for efficient serving, making it suitable for real-world applications.\n",
            "\n",
            "**Responsible Deployment:**\n",
            "\n",
            "* **Impact Assessment:** Google DeepMind emphasizes responsible development and has conducted extensive impact assessments to identify potential benefits and risks associated with Gemini 1.5 Pro.\n",
            "* **Safety Mitigations:** The model undergoes rigorous safety evaluations and incorporates mitigations to reduce potential harms, focusing on aspects like content safety and representational biases.\n",
            "\n",
            "**Overall, Gemini 1.5 Pro represents a significant step forward in AI research, pushing the boundaries of long-context understanding and multimodal capabilities while ensuring responsible development and deployment.**\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tryout"
      ],
      "metadata": {
        "id": "OWSuYAHxVZg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, Part, FinishReason\n",
        "import vertexai.preview.generative_models as generative_models\n",
        "\n",
        "def generate():\n",
        "  vertexai.init(project=\"cloud-llm-preview1\", location=\"us-central1\")\n",
        "  model = GenerativeModel(\"gemini-1.5-pro-preview-0409\")\n",
        "  responses = model.generate_content(\n",
        "      [audio1, \"\"\"Summarize\"\"\"],\n",
        "      generation_config=generation_config,\n",
        "      safety_settings=safety_settings,\n",
        "      stream=True,\n",
        "  )\n",
        "\n",
        "  for response in responses:\n",
        "    print(response.text, end=\"\")\n",
        "\n",
        "audio_file_uri = \"pixel.mp3\"\n",
        "with open(audio_file_uri, 'rb') as file:\n",
        "        binary_data = file.read()\n",
        "        base64_encoded_data = base64.b64encode(binary_data)\n",
        "        encoded_string = base64_encoded_data.decode('utf-8')\n",
        "\n",
        "\n",
        "audio1 = Part.from_data(\n",
        "    mime_type=\"audio/mpeg\",\n",
        "    data=encoded_string)\n",
        "\n",
        "generation_config = {\n",
        "    \"max_output_tokens\": 8192,\n",
        "    \"temperature\": 1,\n",
        "    \"top_p\": 0.95,\n",
        "}\n",
        "\n",
        "safety_settings = {\n",
        "    generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "    generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "    generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "    generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "}\n",
        "\n",
        "generate()\n",
        "\n"
      ],
      "metadata": {
        "id": "AJgzM3uYT86n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8df8ebc2-03cb-43cd-b585-36763cf53d72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The March Pixel Feature Drop brings new features and enhancements to Pixel devices, including:\n",
            "\n",
            "**For Pixel Watch:**\n",
            "\n",
            "* **Pixel Watch 1 gets Pixel Watch 2 features:**  This includes heart rate zone training, pace coaching, automatic workout tracking, and the Fitbit Sleep Profile app.\n",
            "* **Improved Bluetooth connectivity:** Makes it easier to connect previously paired Pixel Buds to your Pixel Watch.\n",
            "\n",
            "**For Pixel Phones:**\n",
            "\n",
            "* **Circle to Search expands to Pixel 7 and 7 Pro:** Allows users to search for anything on their phone from any app or screen.\n",
            "* **10-bit HDR comes to Instagram:** Enables users to take high-quality videos with a wider range of colors and contrast.\n",
            "* **Partial screen sharing:** Allows users to share only a specific app during video calls, enhancing privacy.\n",
            "* **Direct my Call for non-toll-free numbers:** The call screening feature now works with more phone numbers, providing greater convenience and time savings.\n",
            "* **Clear Calling:** Improves call quality by reducing background noise.\n",
            "\n",
            "**For Pixel Tablets:**\n",
            "\n",
            "* **Partial screen sharing:** Similar to phones, allows users to share only a specific app during video calls.\n",
            "* **Docs markup:** Enables users to directly draw on, annotate, and take notes in documents.\n",
            "\n",
            "The March Pixel Feature Drop aims to improve the user experience and make Pixel devices even more helpful and enjoyable. By bringing features from newer models to older ones, Google ensures that users can continue to benefit from the latest advancements. User feedback plays a crucial role in shaping future feature drops, as Google actively listens to user suggestions and strives to address their needs and preferences. The update starts rolling out on March 4th, 2023, and users can look forward to experiencing these new features on their Pixel devices. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f1VFZj-1VgtG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}